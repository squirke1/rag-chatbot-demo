[
  {
    "question": "What is RAG?",
    "ground_truth": "RAG stands for Retrieval-Augmented Generation, an AI framework that combines large language models with external knowledge retrieval to generate more accurate and factual responses."
  },
  {
    "question": "What are the main steps in the RAG process?",
    "ground_truth": "The RAG process includes document ingestion, chunking, embedding, indexing, retrieval, and generation steps."
  },
  {
    "question": "What is MMR and why is it useful?",
    "ground_truth": "MMR (Maximal Marginal Relevance) is a retrieval strategy that balances relevance and diversity to avoid redundant results, useful when you want variety in retrieved context."
  },
  {
    "question": "What are the key evaluation metrics for RAG systems?",
    "ground_truth": "Key metrics include answer relevancy, context relevancy, groundedness, and faithfulness."
  },
  {
    "question": "What is the recommended chunk size for RAG?",
    "ground_truth": "The recommended chunk size is 500-1500 characters with 10-20% overlap to balance context and specificity."
  },
  {
    "question": "What embedding models are commonly used?",
    "ground_truth": "Common embedding models include sentence-transformers/all-MiniLM-L6-v2, OpenAI text-embedding-ada-002, and Cohere embed-english-v3.0."
  },
  {
    "question": "When should you use RAG vs fine-tuning?",
    "ground_truth": "Use RAG when knowledge changes frequently, you need citations, or work with private data. Use fine-tuning when knowledge is stable, latency is critical, or you need to change model behavior."
  },
  {
    "question": "What are some common RAG challenges?",
    "ground_truth": "Common challenges include irrelevant retrieval, hallucination, context window limits, and outdated information."
  },
  {
    "question": "What vector databases are mentioned?",
    "ground_truth": "The document mentions FAISS for local use, Pinecone for cloud, and Weaviate and Qdrant for self-hosted or cloud deployments."
  },
  {
    "question": "What is groundedness in RAG evaluation?",
    "ground_truth": "Groundedness checks if the answer is grounded in the retrieved context rather than hallucinated, ensuring the response is based on actual retrieved information."
  }
]
